{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**\n",
    "``It’s a classification algorithm, that is used where the response variable is categorical. The idea of Logistic Regression is to find a relationship between features and probability of particular outcome.``\n",
    "\n",
    "\n",
    "<img src = 'a_img.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction to Logistic Regression :**\n",
    "\n",
    "Every machine learning algorithm works best under a given set of conditions. Making sure your algorithm fits the assumptions / requirements ensures superior performance. You can’t use any algorithm in any condition. For e.g.: We can’t use linear regression on a categorical dependent variable. Because we won’t be appreciated for getting extremely low values of adjusted R² and F statistic. Instead, in such situations, we should try using algorithms such as Logistic Regression, Decision Trees, Support Vector Machine (SVM), Random Forest, etc.\n",
    "\n",
    "Logistic Regression is a popular statistical model used for binary classification, that is for predictions of the type this or that, yes or no, A or B, etc. Logistic regression can, however, be used for multiclass classification, but here we will focus on its simplest application.\n",
    "\n",
    "As an example, consider the task of predicting someone’s gender (Male/Female) based on their Weight and Height.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression** is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.\n",
    "\n",
    "Logistic Regression is one of the most popular ways to fit models for categorical data, especially for binary response data in Data Modeling. It is the most important (and probably most used) member of a class of models called generalized linear models. Unlike linear regression, logistic regression can directly predict probabilities (values that are restricted to the (0,1) interval); furthermore, those probabilities are well-calibrated when compared to the probabilities predicted by some other classifiers, such as Naive Bayes. Logistic regression preserves the marginal probabilities of the training data. The coefficients of the model also provide some hint of the relative importance of each input variable.\n",
    "\n",
    "``Logistic Regression is used when the dependent variable (target) is categorical.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Types of Logistic Regression:**\n",
    "\n",
    "1. **Binary Logistic Regression**: The categorical response has only two 2 possible outcomes. E.g.: Spam or Not\n",
    "\n",
    "2. **Multinomial Logistic Regression**: Three or more categories without ordering. E.g.: Predicting which food is preferred more (Veg, Non-Veg, Vegan)\n",
    "\n",
    "3. **Ordinal Logistic Regression**: Three or more categories with ordering. E.g.: Movie rating from 1 to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logistic Regression Assumptions:**\n",
    "\n",
    "- Binary logistic regression requires the dependent variable to be binary.\n",
    "\n",
    "- For a binary regression, the factor level 1 of the dependent variable should represent the desired outcome.\n",
    "\n",
    "- Only the meaningful variables should be included.\n",
    "\n",
    "- The independent variables should be independent of each other. That is, the model should have little or no multi-collinearity.\n",
    "\n",
    "- The independent variables are linearly related to the log odds.\n",
    "\n",
    "- Logistic regression requires quite large sample sizes.\n",
    "\n",
    "Even though logistic (logit) regression is frequently used for binary variables (2 classes), it can be used for categorical dependent variables with more than 2 classes. In this case it’s called Multinomial Logistic Regression.\n",
    "\n",
    "<img src = 'b_img.png'  width=\"600\" height=\"400\">\n",
    "<img src = 'c_img.png'  width=\"800\" height=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logit Function**\n",
    "\n",
    "Logistic regression can be expressed as:\n",
    "\n",
    "<img src = 'd_img.png'  width=\"300\" height=\"250\">\n",
    "\n",
    "where, the left hand side is called the logit or log-odds function, and p(x)/(1-p(x)) is called odds.\n",
    "\n",
    "The odds signifies the ratio of probability of success to probability of failure. Therefore, in Logistic Regression, linear combination of inputs are mapped to the log(odds) - the output being equal to 1.\n",
    "If we take an **inverse of the above function**, we get:\n",
    "\n",
    "<img src = 'e_img.png'  width=\"300\" height=\"250\">\n",
    "\n",
    "This is known as the Sigmoid function and it gives an S-shaped curve. It always gives a value of probability ranging from 0<p<1.\n",
    "\n",
    "<img src = 'f_img.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Applications of Logistic Regression:**\n",
    "\n",
    "Logistic regression is used in various fields, including machine learning, most medical fields, and social sciences. For e.g., the Trauma and Injury Severity Score (TRISS), which is widely used to predict mortality in injured patients, is developed using logistic regression. Many other medical scales used to assess severity of a patient have been developed using logistic regression. Logistic regression may be used to predict the risk of developing a given disease (e.g. diabetes; coronary heart disease), based on observed characteristics of the patient (age, sex, body mass index, results of various blood tests, etc.).\n",
    "\n",
    "Another example might be to predict whether an Indian voter will vote BJP or TMC or Left Front or Congress, based on age, income, sex, race, state of residence, votes in previous elections, etc. The technique can also be used in engineering, especially for predicting the probability of failure of a given process, system or product.\n",
    "\n",
    "It is also used in marketing applications such as prediction of a customer’s propensity to purchase a product or halt a subscription, etc. In economics it can be used to predict the likelihood of a person’s choosing to be in the labor force, and a business application would be to predict the likelihood of a homeowner defaulting on a mortgage. Conditional random fields, an extension of logistic regression to sequential data, are used in natural language processing.\n",
    "\n",
    "Logistic Regression is used for prediction of output which is binary. For e.g., if a credit card company is going to build a model to decide whether to issue a credit card to a customer or not, it will model for whether the customer is going to “Default” or “Not Default” on this credit card. This is called “Default Propensity Modeling” in banking terms.\n",
    "\n",
    "Similarly an e-commerce company that is sending out costly advertisement / promotional offer mails to customers, will like to know whether a particular customer is likely to respond to the offer or not. In Other words, whether a customer will be “Responder” or “Non Responder”. This is called “Propensity to Respond Modeling”\n",
    "\n",
    "Using insights generated from the logistic regression output, companies may optimize their business strategies to achieve their business goals such as minimize expenses or losses, maximize return on investment (ROI) in marketing campaigns etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset for Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.5</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0          1        17.99         10.38           122.8     1001.0   \n",
       "1          1        20.57         17.77           132.9     1326.0   \n",
       "2          1        19.69         21.25           130.0     1203.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33            184.6   \n",
       "1         0.1812  ...         24.99          23.41            158.8   \n",
       "2         0.2069  ...         23.57          25.53            152.5   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# membaca dataset \n",
    "data = pd.read_csv(\"cancer.csv\")\n",
    "\n",
    "#mengahapus kolom yang tidak digunakan\n",
    "data.drop([\"Unnamed: 32\",\"id\"], axis=1, inplace=True)\n",
    "\n",
    "# merubah label M (ganas) = 1 dan B (jinak) = 0\n",
    "data.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\n",
    "\n",
    "# menampilkan sample data\n",
    "data.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Spliting Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(['diagnosis'], axis=1)\n",
    "y = data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fitting Model**\n",
    "solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "\n",
    "    Algorithm to use in the optimization problem.\n",
    "\n",
    "-        For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n",
    "\n",
    "-        For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n",
    "\n",
    "-        ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n",
    "\n",
    "-        ‘liblinear’ and ‘saga’ also handle L1 penalty\n",
    "\n",
    "-        ‘saga’ also supports ‘elasticnet’ penalty\n",
    "\n",
    "-        ‘liblinear’ does not support setting penalty='none'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.89361242 -0.20515277 -0.24553851  0.0074053   0.03321902  0.15998303\n",
      "   0.21445026  0.09171891  0.04857537  0.01017327 -0.03335621 -0.40578448\n",
      "  -0.10764678  0.10272549  0.00334861  0.03446109  0.04458414  0.01190454\n",
      "   0.01255993  0.00318775 -0.91634081  0.27399961  0.23891563  0.017254\n",
      "   0.0619081   0.49553281  0.59057666  0.17641094  0.1556528   0.0481576 ]]\n",
      "[-0.16180843]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SONY Z\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(x_train, y_train)\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "\n",
    "m = model.coef_[0][0]\n",
    "c = model.intercept_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediksi\n",
    "y_pred = model.predict(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to predict using model\n",
    "coba = x_test.iloc[:1]\n",
    "model.predict(coba)\n",
    "# coba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42,  1],\n",
       "       [ 1, 13]], dtype=int64)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(8,5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\".0f\", ax=ax)\n",
    "plt.xlabel(\"y Aktual\")\n",
    "plt.ylabel(\"y Prediksi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        43\n",
      "           1       0.93      0.93      0.93        14\n",
      "\n",
      "    accuracy                           0.96        57\n",
      "   macro avg       0.95      0.95      0.95        57\n",
      "weighted avg       0.96      0.96      0.96        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix Report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Take Home Exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Variable Description\n",
    "\n",
    "- Survived: Survived (1) or died (0); this is the target variable\n",
    "- Pclass: Passenger's class (1st, 2nd or 3rd class)\n",
    "- Name: Passenger's name\n",
    "- Sex: Passenger's sex\n",
    "- Age: Passenger's age\n",
    "- SibSp: Number of siblings/spouses aboard\n",
    "- Parch: Number of parents/children aboard\n",
    "- Ticket: Ticket number\n",
    "- Fare: Fare\n",
    "- Cabin: Cabin\n",
    "- Embarked: Port of embarkation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset yang dipakai adalah Titanic\n",
    "- Buat model ML dengan Logistic regression\n",
    "- Evaluasi model menggunakan Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reference**:\n",
    "* Gustavo Chávez, \"Understanding Logistic Regression step by step\", https://towardsdatascience.com/understanding-logistic-regression-step-by-step-704a78be7e0a\n",
    "* Rajesh S. Brid, \"Logistic Regression\", https://medium.com/greyatom/logistic-regression-89e496433063\n",
    "* Apoorva Agrawal, \"Logistic Regression. Simplified.\", https://medium.com/data-science-group-iitr/logistic-regression-simplified-9b4efe801389\n",
    "* Gaurav Chauhan, \"All about Logistic regression in one article\", https://towardsdatascience.com/logistic-regression-b0af09cdb8ad\n",
    "* Dataset: https://www.kaggle.com/c/titanic\n",
    "* Dataset: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python38132bitf9f79e71b62e4503b25567c1d3914456"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
